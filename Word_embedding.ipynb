{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9560ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc81482",
   "metadata": {},
   "source": [
    "### Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b3385e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\ABC\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d22fd75eca4097994988d919c375b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555064cea6ef48acb85e1cc175dbbc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ABC\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteAHUFUG\\imdb_reviews-train.t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ABC\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteAHUFUG\\imdb_reviews-test.tf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\ABC\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteAHUFUG\\imdb_reviews-unsuper…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\ABC\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "imbd,info= tfds.load('imdb_reviews',with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fab221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='C:\\\\Users\\\\ABC\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de272297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for t in imbd['train'].take(3):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4da706",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5683a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=imbd['train'],imbd['test']\n",
    "train_sentence=[]\n",
    "train_label=[]\n",
    "\n",
    "test_sentence=[]\n",
    "test_label=[]\n",
    "\n",
    "for s,l in train_data:\n",
    "    train_sentence.append(s.numpy().decode('utf8'))\n",
    "    train_label.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    test_sentence.append(s.numpy().decode('utf8'))\n",
    "    test_label.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d44d7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels of first 4 is 0\n",
      "labels of first 4 is 0\n",
      "labels of first 4 is 0\n",
      "labels of first 4 is 1\n"
     ]
    }
   ],
   "source": [
    "for l in train_label[:4]:\n",
    "    print(f\"labels of first 4 is {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc05fc6",
   "metadata": {},
   "source": [
    "## Genrate Padded Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34fecc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_Size=10000\n",
    "max_len=120\n",
    "embedding_dim=16\n",
    "trunc='post'\n",
    "oov_tok='<OOV>'\n",
    "\n",
    "tokenizer=Tokenizer(num_words=vocab_Size,oov_token=oov_tok)\n",
    "\n",
    "tokenizer.fit_on_texts(train_sentence)\n",
    "train_sequence=tokenizer.texts_to_sequences(train_sentence)\n",
    "test_sequence=tokenizer.texts_to_sequences(test_sentence)\n",
    "\n",
    "train_sequence=pad_sequences(train_sequence,maxlen=max_len,truncating=trunc)\n",
    "test_sequence=pad_sequences(test_sequence,maxlen=max_len,truncating=trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcc5e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(train_label)\n",
    "testing_labels_final = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef433d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0   12   14   33  425  392   18   90   28    1    9   32\n",
      "  1366 3585   40  486    1  197   24   85  154   19   12  213  329   28\n",
      "    66  247  215    9  477   58   66   85  114   98   22 5675   12 1322\n",
      "   643  767   12   18    7   33  400 8170  176 2455  416    2   89 1231\n",
      "   137   69  146   52    2    1 7577   69  229   66 2933   16    1 2904\n",
      "     1    1 1479 4940    3   39 3900  117 1584   17 3585   14  162   19\n",
      "     4 1231  917 7917    9    4   18   13   14 4139    5   99  145 1214\n",
      "    11  242  683   13   48   24  100   38   12 7181 5515   38 1366    1\n",
      "    50  401   11   98 1197  867  141   10]\n",
      " [   0    0    0    0    0    0    0    0   11   26   75  571    6  805\n",
      "  2354  313  106   19   12    7  629  686    6    4 2219    5  181  584\n",
      "    64 1454  110 2263    3 3951   21    2    1    3  258   41 4677    4\n",
      "   174  188   21   12 4078   11 1578 2354   86    2   20   14 1907    2\n",
      "   112  940   14 1811 1340  548    3  355  181  466    6  591   19   17\n",
      "    55 1817    5   49   14 4044   96   40  136   11  972   11  201   26\n",
      "  1046  171    5    2   20   19   11  294    2 2155    5   10    3  283\n",
      "    41  466    6  591    5   92  203    1  207   99  145 4382   16  230\n",
      "   332   11 2486  384   12   20   31   30]\n",
      " [4383 6109    2    1 4802 3976    9    4  895 1595    3 2003 1341    3\n",
      "  2339 8927  200  735  352   15   34  208  304    6   79    8    8   19\n",
      "   214   21  360    4    1  977    2   82    5 3977  541    1    6    1\n",
      "   530    4    1  420    4    1    3    6    1    2 1161  530   93    1\n",
      "  8059   10   44   21    2 1984   16 1136    5    2  511    8    8  162\n",
      "    58 2589 7263   13  572   21    2 2271  497    5    2 3607  313    2\n",
      "     1 1812 3397  441 4383    3 1167  973    6   26 4045 3977  541   16\n",
      "     1    2 2271 2388   16    2  296 1356 1261    8    8 2271  795   28\n",
      "  2838   16    4    1 2998  550    5  735]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequence[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e33afc",
   "metadata": {},
   "source": [
    "## Buidl A model and Complie "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9a14f",
   "metadata": {},
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33168bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=layers.Input(max_len)\n",
    "x=layers.Embedding(vocab_Size,embedding_dim)(inp)\n",
    "x=layers.Flatten()(x)\n",
    "x=layers.Dense(6,activation='relu')(x)\n",
    "x=layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model=tf.keras.Model(inp,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc0e6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 120)]             0         \n",
      "                                                                 \n",
      " embedding_11 (Embedding)    (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61872348",
   "metadata": {},
   "source": [
    "### Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c5d8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_Size, embedding_dim, input_length=max_len),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be4d326d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4862 - accuracy: 0.7520 - val_loss: 0.3966 - val_accuracy: 0.8176\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2355 - accuracy: 0.9108 - val_loss: 0.4116 - val_accuracy: 0.8210\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0906 - accuracy: 0.9770 - val_loss: 0.5078 - val_accuracy: 0.8092\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9970 - val_loss: 0.6181 - val_accuracy: 0.8029\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9996 - val_loss: 0.6859 - val_accuracy: 0.8089\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7444 - val_accuracy: 0.8077\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 8.0817e-04 - accuracy: 1.0000 - val_loss: 0.7958 - val_accuracy: 0.8084\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 4.4644e-04 - accuracy: 1.0000 - val_loss: 0.8432 - val_accuracy: 0.8078\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 2.6094e-04 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.8083\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 1.5819e-04 - accuracy: 1.0000 - val_loss: 0.9323 - val_accuracy: 0.8075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e949c2ce50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequence, training_labels_final, epochs=num_epochs, validation_data=(test_sequence, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7729d",
   "metadata": {},
   "source": [
    "### Visualize Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3ce1e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer=model.layers[0]\n",
    "embedding_weights=embedding_layer.get_weights()[0]\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3741f",
   "metadata": {},
   "source": [
    "You will need to generate two files:\n",
    "\n",
    "* vecs.tsv - contains the vector weights of each word in the vocabulary\n",
    "* meta.tsv - contains the words in the vocabulary\n",
    "* For this, it is useful to have reverse_word_index dictionary so you can quickly lookup a word based on a given index. For * example, reverse_word_index[1] will return your OOV token because it is always at index = 1. Fortunately, the Tokenizer class alre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d35424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "reverse_word_index = tokenizer.index_word\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_Size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "078b8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d3c78",
   "metadata": {},
   "source": [
    "### 'https://projector.tensorflow.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8218e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
